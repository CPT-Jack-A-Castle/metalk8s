Some notes
==========
- The intent is for this installer to deploy a system which looks exactly like
one deployed using `kubeadm`, i.e. using the same (or at least highly similar)
static manifests, cluster `ConfigMap`s, RBAC roles and bindings, ...

The rationale: at some point in time, once `kubeadm` gets easier to embed in
larger deployment mechanisms, we want to be able to switch over without too much
hassle.

Also, `kubeadm` applies best-practices so why not follow them anyway.

What the different components are
---------------------------------

How do we distribute this installer ?
They should be gathered into a single archive of some sort, like an ISO

First we need to distinguish the various component that might need to be
brought together into this single archive:


The platform layer itself (Metalk8s)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Package repositories
""""""""""""""""""""

The platform layer will need some packages for the initial phase of the
deployment (likely K8s ones like kubelet, etcd, etc ...) but also for the
tools required for the whole bootstrap phase.
Ideally, we will have a repository for each purpose to more easily
maintain them in the future offline environment

K8s component repository
''''''''''''''''''''''''

SPEC file located at
https://github.com/kubernetes/release/blob/master/rpm/kubelet.spec
can be used as a reference for building needed K8s system packages


EPEL dependencies
'''''''''''''''''

All EPEL dependencies needed by the package we provide should be in
this repository.


Hardware specific related packages
''''''''''''''''''''''''''''''''''

Packages that we vendor related to specific hardware, like ssacli,
perccli, storcli should be present here.


CentOS minimal repository
'''''''''''''''''''''''''

A copy with at least the yum group "Infrastruture Server" should be
present here to ensure an offline environment with the minimal set of
packages required for the default installation.

  .. code:: bash

PACKAGES+=($(repoquery -g --grouppkgs=all --list -g base core))

repotrack -n -t -p "$REPO" -a x86_64 "${PACKAGES[@]}"



MetalK8s repository
""""""""""""""""""""""""""

This repository will contain all MetalK8s related packages, notably

  * MetalK8s bootstrap package

  A system package respecting SemVer will contain bootstrap related files
  such as the initial bootstrap script.
  This package will be the first thing installed onec the MetalK8s installer is
  deployed and available on the bootstrap node.

  .. code:: bash

    yum localinstall /srv/scality/repositories/metalk8s/v2.0/metalk8s-2.0.0-1.el7.x86_64.rpm


  * Possibly other MetalK8s related packages


Docker registry
"""""""""""""""

A docker registry like service should be made available as part of the
bootstrap.
This will allow new solutions deployed to be made available to the
orchestrator by pulling the images from this registry



Configuration
-------------
To launch the bootstrap process, some input from the end-user is required, which
can vary from one installation to another:

- CIDR (i.e. `x.y.z.w/n`) of the control plane networks to use

  Given these CIDR, we can find the address on which to bind services like
  `etcd`, `kube-apiserver`, `kubelet`, `salt-master` and others.

  These should be existing networks in the infrastructure to which all hosts
  are connected.

  This is a list of CIDRs, which will be tried one after another, to find a
  matching local interface (i.e. hosts comprising the cluster may reside in
  different subnets, e.g. control plane in VMs, workload plane on physical
  infrastructure).

- CIDRs (i.e. `x.y.z.w/n`) of the workload plane networks to use

  Given these CIDRs, we can find the address to be used by the CNI overlay
  network (i.e. Calico) for inter-`Pod` routing.

  This can be the same as the control plane network.

- CIDR (i.e. `x.y.z.w/n`) of the `Pod` overlay network

  Used to configure the Calico `IPPool`. This must be a non-existing network in
  the infrastructure.

  Default: `10.233.0.0/16`

- CIDR (i.e. `x.y.z.w/n`) of the `Service` network

  Default: `10.96.0.0/12`

- VIP for the `kube-apiserver` and `keepalived` toggle

  Used as the address of `kube-apiserver` where required. This can either be a
  VIP managed by custom load-balancing/high-availability infrastructure, in
  which case the `keepalived` toggle must be off, or one which our platform will
  manage using `keepalived`.

  If `keepalived` is enabled, this VIP must sit in a control plane CIDR shared
  by all control plane nodes.

  Note: we run `keepalived` in unicast mode, which is an extension of classic
  VRRP, but removes the need for multicast support on the network.

Firewall
--------
We assume a host-based firewall is used, based on `firewalld`. As such, for any
service we deploy which must be accessible from the outside, we must set up an
appropriate rule.

We assume SSH access is not blocked by the host-based firewall.

These services include:

- VRRP if `keepalived` is enabled on control-plane nodes
- HTTPS on the bootstrap node, for `nginx` fronting the OCI registry and serving
  the yum repository
- `salt-master` on the bootstrap node
- `etcd` on control-plane / etcd nodes
- `kube-apiserver` on control-plane nodes
- `kubelet` on all cluster nodes

@startuml

actor user
entity bootstrap

== Initialization ==

user -> bootstrap : Upload installer
user -> bootstrap : Run installer

bootstrap -> bootstrap : Unpack installer static files and install script
create control installer
bootstrap -> installer : Run

== Bootstrap ==

installer -> bootstrap : Run pre-minion local checks (e.g. OS release)
installer -> bootstrap : Disable salt-minion service || true
installer -> bootstrap : Stop salt-minion service || true


installer -> bootstrap : Install salt-minion and dependencies from unpacked RPMs
installer -> bootstrap : Create salt-minion configuration file to run 'local'

installer -> bootstrap : Run bootstrap node pre-checks (salt-call --local)

group Initialize CRI/CNI/Kubelet environment
installer -> bootstrap : Run CRI/CNI role using salt-call --local
bootstrap -> bootstrap : Install container-selinux, runc, containerd, cri-tools
bootstrap -> bootstrap : Create /etc/crictl.conf
bootstrap -> bootstrap : Enable and start containerd
bootstrap -> bootstrap : Install ebtables, socat, kubernetes-cni, kubelet
bootstrap -> bootstrap : Create initial kubelet configuration file
bootstrap -> bootstrap : Create kubelet systemd drop-in to set CRI endpoint and config file, enable CPU/memory accounting
bootstrap -> bootstrap : Enable and start kubelet
bootstrap --> installer : Done
end

group Set up Kubernetes control plane HA/failover
note over bootstrap: TODO running in container using static kubelet manifest
end

group Set up salt-master
installer -> bootstrap : Deploy salt-master using salt-call --local
bootstrap -> bootstrap : Create salt-master static pod manifest
note right of bootstrap: Can we do an upgrade of salt-master here? What about nodes with older minion versions?

create control saltmaster

bootstrap -> saltmaster : Wait to be ready
saltmaster --> bootstrap : Ready

bootstrap --> installer : Done
end

installer -> bootstrap : Remove salt-minion 'local' config
installer -> bootstrap : Enable salt-minion service
installer -> bootstrap : Start salt-minion service

create control saltminion
saltmaster <-> saltminion : Hello

== Deploy bootstrap node ==

saltmaster -> saltminion : Install bootstrap node

saltminion -> saltminion : Inject OCI registry image

alt if newer OCI registry version
saltminion -> saltminion : Remove OCI registry manifest
saltminion -> saltminion : Wait for OCI registry to be down
saltminion -> saltminion : Create OCI registry manifest TODO CA
note left: Bound to 127.0.0.1 only
end

saltminion -> saltminion : Wait for OCI registry to be up
saltminion -> saltminion : Inject images in OCI registry

saltminion -> saltminion : Drop nginx manifest in place

saltminion --> saltmaster : Done

note over bootstrap
At this point, the bootstrap node hosts

- salt-master
- an nginx service serving a yum repository
- an OCI image registry (proxied by nginx)
end note

== Deploy control plane ==

installer -> saltmaster : Deploy control plane on bootstrap unless one exists
saltmaster -> saltminion : Go

installer -> saltmaster : Deploy UI
saltmaster -> saltminion : Go
saltminion --> saltmaster : Done
saltmaster --> installer : Done
installer --> bootstrap : Done

bootstrap --> user : UI ready at ...

== Extend control plane ==

user -> bootstrap : Add control-plane node

create entity leader

bootstrap -> leader : salt-ssh install salt-minion
leader -> bootstrap : Done

== Extend worker plane ==

user -> bootstrap : Add worker node

create entity node

bootstrap -> node : salt-ssh install salt-minion
node -> bootstrap : Done

@enduml
