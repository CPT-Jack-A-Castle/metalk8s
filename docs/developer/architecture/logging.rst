Logging
=======

This section summarizes centralized logging design architecture which is closely
linked to the above requirements (to be merged).

The following terms will be used throughout the context of this document so it
is fair that we describe their meanings.

Collector/Shipper
-----------------
A log Collector allows you to collect logs from different sources, unify
and send them to multiple destinations. An example of such a system is the
Linux utility Rsyslog

Distributor
-----------
A log Distributor is a utility capable of adding metadata to logs and
forwarding these logs to unique sources.
The metadata (labels) set on log streams are used by log visualizers for
querying purposes. An example of such system is Logstash.

Visualizer
----------
A log Visualizer allows one to query and visualize logs. This can be possible
mainly because of the metadata set on logs by log Distributors. Note also that a
log Visualizer is not designed to perform log indexing duties. An example
of such a system is Kibana.


Log Shipper choices
+++++++++++++++++++

The MetalK8s log shipper design choice is based on the following capabilities
    - Ability to collect and output to multiple sources
    - Log type could be filesystem logs, common log files like systemd, kubelet,
      journald etc
    - Log shipper should not bring extra performance penalties
    - JSON format is good but we have other sources of logs e.g plaintext and
      so format issues for unstructured or structure logs should not be an issue

Log shipper industry top players:
+++++++++++++++++++++++++++++++++

1. Fluent bit
    - light weight
    - written in C
    - Optimistic low memory and CPU usages (ideal for MetalK8s workloads)
    - Plugin-based architecture with over 50+ plugins
    - Guarantee to transfer logs with security (TLS encrypted)

Internals of Fluentbit:
    - Made up of an input, parser, filter, buffer, then routing or output plugins
    - Ability to add context to every event that is generated ie can enrich the
      logs with Kubernetes metadata
    - Ability to expose metric directly to Prometheus (serviceMonitors)
    - Ability to send one record to multiple backends e.g Elasticsearch, Splunk, http only endpoints
    - Ability to implement parsers from collector system (parsing at source)
    - Ability to provide a stream processor engine ensuring we can run
      out of memory forecast for a specific node in MetalK8s and generate future
      alerts based on this information.

2. Promtail
    - Designed to read and send log streams directly to Loki
    - Can ensure a match-up between metrics (Prometheus) and logs (Loki) using labels
    - Ability to allow Prometheus to scrap metrics
    - Ability to label logs before shipping them
    - Ability to tail logs from all files given a host path
    - Ability to extract metrics from logs such as counting the occurrences of
      a particular message

    Limitations to Promtail worth mentioning
      - No option to drop events
      - No option to parse the entry logs.
      - No option to ship logs to other sources meaning external systems will
        need to deploy their own independent log collectors.

3. PaStash
    - A NodeJS multi I/O processor supporting ingestion and correlation of logs.
    - Has lower memory footprint
    - Has lower cpu footprint
    - Supports the Loki output format alongside many others
      For more, see [here](https://github.com/sipcapture/paStash)

4 Unexploited choices
    - Rsyslog
    - Filebeat
    - Logstash
    - Logtail

Requiring a log shipper which can be configured to ship logs to multiple
backends, is lightweight and consumes fewer resources on a host tilts the design
choice likely towards a utility like Fluentbit.

Log Collection Modes
++++++++++++++++++++

Two main log collection modes exist which are; DaemonSet mode,
and SideCar mode.

1. DaemonSet mode
A log shipper is deployed on every Kubernetes node. The log shipper collects
the container logs and sends them to a designated endpoint. Any new node added
to the cluster automatically gets a log shipper pod.

2. SideCar mode
Each Pod runs a SideCar log shipper container to collect logs generated by the
primary container in the Pod. Highly customizable and flexible since every
log shipper per pod can be configured separately but resource utilization
becomes drastically high in this approach.

Log Collection mode choice
++++++++++++++++++++++++++

Because **minimal resource consumption** for the choosen log shipper is key for
both workload and control-plane nodes the designated log collection mode is
seemingly the DaemonSet mode. This implies we prioritize the log
shipper resource consumption rates over the ease of customization and
flexibility.


Log Distributor of choice
+++++++++++++++++++++++++

Several utilities out there can play the role of a log distributor in MetalK8s
but this section aims to convince others why Fluentd and not Promtail is a
potential distributor of choice.

In a MetalK8s cluster, the Log Distributor ideally is set up to receive, parse
and forward logs to designated backends.

Fluentd
-------

- Provides the ability to expose metrics to Prometheus
- Can integrate properly with a log shipper like Fluentbit meaning we can
  keep a light-weight shipper
- Can be configured to ship to multiple backends e.g Elasticsearch, Loki, Splunk
  For more plugins see [here](https://www.fluentd.org/plugins)
- Supports a plugin-like architecture e.g
    - fluent-plugin-tag-normaliser plugin help re-tag logs with Kubernetes
      metadata coming from fluent-bit
    - fluent-plugin-label-router plugin can route logs based on their
      Kubernetes metadata to different backends
    - fluent-plugin-grafana-loki plugin can ship logs to a Grafana Loki server

      And so on...

- Ability to provide log parsing functionalities. Parsing logs at source could
  be resource-consuming hence MetalK8s infra nodes could run the parsers and
  forward these logs to the desired backend. e.g Loki
- Offers autoscaling using HorizontalPodAutoscaler which is configurable
  meaning during burst periods, scaling more replicas to handle load is easier
  for admins
- Provides a buffering mechanism that ensures log events can be buffered to
  to disk if receivers are unavailable. This guarantees reliability and logs
  are not lost in whatever case.
- Log parsers are configurable via ConfigMap so can be passed as MetalK8s
  service configurations and hence upgrade-able in any instance.

Unexploited choices
-------------------

- Logstash


Why Grafana Loki
++++++++++++++++

Many other log visualizers exist like Kibana and Graylog but Loki is chosen
for the following simple reasons;

- Fits well into the MetalK8s monitoring stack since we already ship Grafana
  as part of our monitoring stack
- Loki offers indexing and grouping of logs based on labels set by log
  Distributors as seen above.
- Enables a switch between metric and logs in a common place ensuring MetalK8s
  admins can troubleshoot easily in one place.

Storage Retention
-----------------

Loki supports storing indexes and chunks in table-based data storages.
Retention in Loki is handled through a Table Manager.

Source: Grafana Loki official Documentation
The Table Manager is a Loki component which takes care of creating periodic
indexes and deleting it once its data time range exceeds the retention period.

Time-based retention
++++++++++++++++++++

To solve the time-based retention period of logs, we need to configure Loki to
enable deletions and retention periods. This will likely be accomplished using
service configurations to enable admins to modify this information at any point
in time. Default time-based values for retention period will be set based on the
above requirements.
The above proposition solves the requirements for a time-based retention period
for logs meaning MetalK8s Admins can keep logs for a period of 2 weeks.

Volume-based retention
++++++++++++++++++++++

To solve the issue of not losing logs because of filled Volumes, requires that
we are able to implement for example a CRON process that calculates the Volume
usages then delete old log chunks on Volumes to avoid filling up disk entirely.

Loki does not offer the capability of log chunk deletion via an API as from this
[issue](https://github.com/grafana/loki/issues/577) so having an independent
process that deletes old log chunks will not work.

Alternatively, should the Buffer system of the Log distributor kick-in once
a receiver has no more write capacity? But for how long?

Saidly, Promtail has no buffer system, one more reason to consider Fluentd.

OR

Alternatively, MetalK8s could reconfigure the Time-based retention period
basing its calculations on certain properties/triggers.

Random ideas...

- Have an Operator that watches the Volume usage of the Log indexing node and
  dynamically reconfigures the time-based retention period which most likely
  will be a Service Configuration in MetalK8s.

- Run a Fluentbit stream processor on this node, that performs forecast on
  Volume usages and then find a way to reconfigure the Time-based retention
  period based on the log ingestion forecast.

- What else?

Scaling the service
+++++++++++++++++++
