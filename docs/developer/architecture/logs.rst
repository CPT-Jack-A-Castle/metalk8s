Log Centralization
==================

Context
-------

MetalK8s value is to provide, out of the box, some services in order to ease
the monitoring and operation of the platform as well as workloads running on
top of it.

It currently provides monitoring service, powered by Prometheus and
AlertManager, in order to expose metrics and alerts for both control-plane and
workload-plane components as well as for cluster nodes HW and OS.

The logs generated by the platform and the workloads constitute an essential
piece of information when it comes to understanding the root cause of a failure
or a performance degradation. Because of the distributed nature of MetalK8s and
workloads running on top of it, the administrators need some tooling to ease
the analysis of near real time and past logs, from a central endpoint. As such
these logs should be stored on the platform, for a configurable period.
Browsing the logs is accessible through an API and a UI. The UI should ease
correlation between logs and health/performance KPIs as well as alerts.

For organisations having their own Log centralization system (like Splunk or
Elasticsearch), MetalK8s should provide some documentation in order guide the
customer to deploy and configure its own log collection agent.

The following requirements focus on application logs. Audit logs are not part
of the requirements.

Requirements
------------

*Lightweight*

A lightweight tool to store and expose logs is required in order to minimize
the HW footprints (CPU, RAM, Disks):

- limited history: Storing the logs for very large period (3 years) is not
  something metalK8s needs to provide as a feature. This can be achieved using
  external log centralization system.

- stable ingestion: It is important to guarantee stable ingestion of the logs
  and less important to guarantee stable performances when browsing/searching
  the logs. However, peak loads related to complex logs queries should not
  impact the application workloads and deploying log storage and search service
  on infra nodes might help achieving this isolation.

- stream indexing: It is not required to have automatic indexing of logs
  content. Instead, the log centralization service should offer basic features
  to group/filter logs per tag/metadata defining the log stream.

*Accessible from a central UI/API*

Platform Admin or Storage Admin can visualize logs from all containers in all
namespaces as well as journal logs, including (kubelet, containerd, salt-minion
, kernel, initrd, services, etc ...) in Grafana. One can correlate logs and
metrics or alerts in one single Grafana Dashboard.
Browsing logs can be achieved through a documented API in order to expose logs
in MetalK8s UIs or other workloads UIs if needed.

*Persistence, Retention*

Logs should be stored on a persistent storage. Platform Administrator can
configure a max retention period. Some automatic purging mechanism is triggered
when logs are older than the retention period or when the persistent storage is
about to reach its capacity limit. Purging jobs are logged.
A typical and default retention period is of 2 weeks.
A formula can be used by solution developers in order to properly size the
persistent storage for log centralization (cf documentation requirement).

*Horizontally scalable (capacity and performance)*

The Platform Administrator can scale the service in order to ingest/query and
store more logs. It can be because more workloads are running on the platform
or because there is a need to keep bigger history of logs.

*Highly Available*

Log collection, ingestion, storage and query services can be replicated in
order to ensure that we can lose at least one server in the cluster without
impacting availability and reliability of the service.

*Log Querying*

Get all logs for a given period, node(s), pod regex, limited list of predefined
labels and free keywords text.
Typical Zenko use case: collecting all logs across several components, related
to a S3 uniq request.
Typical predefined labels are severity and namespace.

*Log statistics (nice to have)*

The Log centralization service also offers the ability to consume statistics
about the logs like the number of occurrences of one type of log during a
certain period of time.

*Monitorable/Observable service (health, performances and alerts)*

The Platform Administrator can monitor capacity usage, ingestion rate, IOPS,
latency and bandwidth of the Log centralization service. He can also monitor
the health of the service (i.e. if some active alerts exist).
He is notified through an alert notification when the service is degraded or
unavailable. It can be because the persistent storage is full or unhealthy or
because the service does not manage to ingest logs at the requested pace.

Here are few example of situations we would like to detect through those KPIs:

- a workload generating a crazy amount of logs
- a burst of ingested logs
- the log persistent storage getting full
- very slow api responses (impacting usability in Grafana dashboards)
- the ingestion of logs working too slowly

*Performances (TBD)*

Typical workloads can generate around 1000 logs per second per node.

User Stories
------------

- As a Platform Administrator, I want to browse all MetalK8s containers logs
  (from all servers) from a unique endpoint, in order to ease distributed
  K8s and MetalK8s services error investigation.

- As a Platform Administrator, I want to browse non container (kubelet,
  containerd, salt-minion, cron) logs (from all servers) from a
  unique endpoint, in order to ease System error investigation.

- As a Storage Administrator, I want to browse all Solution instance containers
  logs (from all servers) from a unique endpoint, in order to ease Solution
  instance error investigation.

- As a Platform Administrator, I want to push all containers logs to an
  external log centralization system, In Order to archive it or aggregate it
  with other application logs.


(some other US extracted from Loki design doc)

- After receiving an alert on my service and drilling into the query associated
  with said alert, I want to quickly see the logs associated with the jobs
  which produced those timeseries at the time of the alert.

- After a pod or node disappears, I want to be able to retrieve logs from just
  before it died, so I can diagnose why it died.

- After discovering an ongoing issue with my service, I want to extract a
  metric from some logs and combine it with my existing time series data.

- I have a legacy job which does not expose metrics about errors - it only logs
  them. I want to build an alert based on the rate of occurrences of errors in
  the log.


Deployment & Configuration
--------------------------

The log centralization storage service is scheduled on infra nodes.
A platform Administrator can operate the service as follows:

- add persistent storage
- configure max retention period
- adjust the number of replicas
- configure the system so that logs are pushed to an external log
  centralization service
- configure log service alerts (IOPS or ingestion rate, latency, bandwidth,
  capacity usage) i.e. adjust the thresholds, silence some alerts or configure
  notifications.

Those operations are accessible from any host able to access the control plane
network and are exposed through the centralised cli framework.

When installing or upgrading MetalK8s, the log centralization service is
automatically scheduled (as soon as a persistent volume is provisioned) on one
infra node.

All configurations of the log centralization service are part of the MetalK8s
backup and remains unchanged when performing an upgrade.

During future MetalK8s upgrades, the service stays available (when replicated).

Monitoring
----------

- An alert rule is fired when the log centralization service is not healthy
- The log centralization service is not healthy when log storage is getting
  full or when service is not able to ingest logs at the right pace.
- IOPS, bandwidth, latency, capacity usage KPIs are available in Prometheus

UI
--

Logs can be seen in Grafana.
Log centralization monitoring information are displayed in the MetalK8s UI
overview page.
A Grafana dashboard gathering health/performance KPIs, as well as alerts for
log centralization service is available when deploying/upgrading MetalK8s.

Documentation
-------------

The sizing section in Introduction page is updated to include log
centralization service impact. The sizing rule takes in account the retention
period, workloads expected log rate and workload predefined indices. This rule
is to be known by solution developers to properly size the service based on
the workload properties.

The Post Installation page is updated to indicate that persistent storage is
needed for log centralization service.

A new page should be added to explains how to operate the service.

The Cluster Monitoring page is updated to describe the log centralization
service.
