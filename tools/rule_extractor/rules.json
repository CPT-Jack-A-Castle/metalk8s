{
    "data": {
        "groups": [
            {
                "evaluationTime": 0.000558974,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-alertmanager.rules.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:00.310893583Z",
                "name": "alertmanager.rules",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "The configuration of the instances of the Alertmanager cluster `{{ $labels.namespace }}/{{ $labels.service }}` are out of sync.\n{{ range printf \"alertmanager_config_hash{namespace=\\\"%s\\\",service=\\\"%s\\\"}\" $labels.namespace $labels.service | query }}\nConfiguration hash for pod {{ .Labels.pod }} is \"{{ printf \"%.f\" .Value }}\"\n{{ end }}"
                        },
                        "duration": 300,
                        "evaluationTime": 0.000367998,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:00.310902398Z",
                        "name": "AlertmanagerConfigInconsistent",
                        "query": "count by(namespace, service) (count_values by(namespace, service) (\"config_hash\", alertmanager_config_hash{job=\"prometheus-operator-alertmanager\",namespace=\"metalk8s-monitoring\"})) != 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "Reloading Alertmanager's configuration has failed for {{ $labels.namespace }}/{{ $labels.pod}}."
                        },
                        "duration": 600,
                        "evaluationTime": 7.9758e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:00.311271738Z",
                        "name": "AlertmanagerFailedReload",
                        "query": "alertmanager_config_last_reload_successful{job=\"prometheus-operator-alertmanager\",namespace=\"metalk8s-monitoring\"} == 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "Alertmanager has not found all other members of the cluster."
                        },
                        "duration": 300,
                        "evaluationTime": 9.8031e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:00.311352083Z",
                        "name": "AlertmanagerMembersInconsistent",
                        "query": "alertmanager_cluster_members{job=\"prometheus-operator-alertmanager\",namespace=\"metalk8s-monitoring\"} != on(service) group_left() count by(service) (alertmanager_cluster_members{job=\"prometheus-operator-alertmanager\",namespace=\"metalk8s-monitoring\"})",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.025938145,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-etcd.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:15.280334546Z",
                "name": "etcd",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "etcd cluster \"{{ $labels.job }}\": members are down ({{ $value }})."
                        },
                        "duration": 180,
                        "evaluationTime": 0.000622336,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:15.280343008Z",
                        "name": "etcdMembersDown",
                        "query": "max by(job) (sum by(job) (up{job=~\".*etcd.*\"} == bool 0) or count by(job, endpoint) (sum by(job, endpoint, To) (rate(etcd_network_peer_sent_failures_total{job=~\".*etcd.*\"}[3m])) > 0.01)) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "etcd cluster \"{{ $labels.job }}\": insufficient members ({{ $value }})."
                        },
                        "duration": 180,
                        "evaluationTime": 0.00018724,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:15.280967146Z",
                        "name": "etcdInsufficientMembers",
                        "query": "sum by(job) (up{job=~\".*etcd.*\"} == bool 1) < ((count by(job) (up{job=~\".*etcd.*\"}) + 1) / 2)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "etcd cluster \"{{ $labels.job }}\": member {{ $labels.instance }} has no leader."
                        },
                        "duration": 60,
                        "evaluationTime": 8.8581e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:15.281155332Z",
                        "name": "etcdNoLeader",
                        "query": "etcd_server_has_leader{job=~\".*etcd.*\"} == 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "etcd cluster \"{{ $labels.job }}\": {{ $value }} leader changes within the last 15 minutes. Frequent elections may be a sign of insufficient resources, high network latency, or disruptions by other components and should be investigated."
                        },
                        "duration": 300,
                        "evaluationTime": 0.000259516,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:15.281244558Z",
                        "name": "etcdHighNumberOfLeaderChanges",
                        "query": "increase((max by(job) (etcd_server_leader_changes_seen_total{job=~\".*etcd.*\"}) or 0 * absent(etcd_server_leader_changes_seen_total{job=~\".*etcd.*\"}))[15m:1m]) >= 3",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "etcd cluster \"{{ $labels.job }}\": {{ $value }}% of requests for {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}."
                        },
                        "duration": 600,
                        "evaluationTime": 0.011604889,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:15.281504993Z",
                        "name": "etcdHighNumberOfFailedGRPCRequests",
                        "query": "100 * sum by(job, instance, grpc_service, grpc_method) (rate(grpc_server_handled_total{grpc_code!=\"OK\",job=~\".*etcd.*\"}[5m])) / sum by(job, instance, grpc_service, grpc_method) (rate(grpc_server_handled_total{job=~\".*etcd.*\"}[5m])) > 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "etcd cluster \"{{ $labels.job }}\": {{ $value }}% of requests for {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}."
                        },
                        "duration": 300,
                        "evaluationTime": 0.011439814,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:15.293114335Z",
                        "name": "etcdHighNumberOfFailedGRPCRequests",
                        "query": "100 * sum by(job, instance, grpc_service, grpc_method) (rate(grpc_server_handled_total{grpc_code!=\"OK\",job=~\".*etcd.*\"}[5m])) / sum by(job, instance, grpc_service, grpc_method) (rate(grpc_server_handled_total{job=~\".*etcd.*\"}[5m])) > 5",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "etcd cluster \"{{ $labels.job }}\": gRPC requests to {{ $labels.grpc_method }} are taking {{ $value }}s on etcd instance {{ $labels.instance }}."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000281808,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:15.304558311Z",
                        "name": "etcdGRPCRequestsSlow",
                        "query": "histogram_quantile(0.99, sum by(job, instance, grpc_service, grpc_method, le) (rate(grpc_server_handling_seconds_bucket{grpc_type=\"unary\",job=~\".*etcd.*\"}[5m]))) > 0.15",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "etcd cluster \"{{ $labels.job }}\": member communication with {{ $labels.To }} is taking {{ $value }}s on etcd instance {{ $labels.instance }}."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000114584,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:15.304841659Z",
                        "name": "etcdMemberCommunicationSlow",
                        "query": "histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 0.15",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "etcd cluster \"{{ $labels.job }}\": {{ $value }} proposal failures within the last 30 minutes on etcd instance {{ $labels.instance }}."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000104432,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:15.30495695Z",
                        "name": "etcdHighNumberOfFailedProposals",
                        "query": "rate(etcd_server_proposals_failed_total{job=~\".*etcd.*\"}[15m]) > 5",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "etcd cluster \"{{ $labels.job }}\": 99th percentile fync durations are {{ $value }}s on etcd instance {{ $labels.instance }}."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000275148,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:15.305062031Z",
                        "name": "etcdHighFsyncDurations",
                        "query": "histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 0.5",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "etcd cluster \"{{ $labels.job }}\": 99th percentile commit durations {{ $value }}s on etcd instance {{ $labels.instance }}."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000319036,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:15.305338166Z",
                        "name": "etcdHighCommitDurations",
                        "query": "histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 0.25",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "{{ $value }}% of requests for {{ $labels.method }} failed on etcd instance {{ $labels.instance }}"
                        },
                        "duration": 600,
                        "evaluationTime": 0.000178305,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:15.305658156Z",
                        "name": "etcdHighNumberOfFailedHTTPRequests",
                        "query": "sum by(method) (rate(etcd_http_failed_total{code!=\"404\",job=~\".*etcd.*\"}[5m])) / sum by(method) (rate(etcd_http_received_total{job=~\".*etcd.*\"}[5m])) > 0.01",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "{{ $value }}% of requests for {{ $labels.method }} failed on etcd instance {{ $labels.instance }}."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000214798,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:15.305837224Z",
                        "name": "etcdHighNumberOfFailedHTTPRequests",
                        "query": "sum by(method) (rate(etcd_http_failed_total{code!=\"404\",job=~\".*etcd.*\"}[5m])) / sum by(method) (rate(etcd_http_received_total{job=~\".*etcd.*\"}[5m])) > 0.05",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "etcd instance {{ $labels.instance }} HTTP requests to {{ $labels.method }} are slow."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000210448,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:15.306053844Z",
                        "name": "etcdHTTPRequestsSlow",
                        "query": "histogram_quantile(0.99, rate(etcd_http_successful_duration_seconds_bucket[5m])) > 0.15",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.001073618,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-general.rules.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:25.520828673Z",
                "name": "general.rules",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "{{ printf \"%.4g\" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000733871,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:25.520838302Z",
                        "name": "TargetDown",
                        "query": "100 * (count by(job, namespace, service) (up == 0) / count by(job, namespace, service) (up)) > 10",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [
                            {
                                "activeAt": "2020-11-27T17:37:25.520605063Z",
                                "annotations": {
                                    "message": "This is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty."
                                },
                                "labels": {
                                    "alertname": "Watchdog",
                                    "severity": "none"
                                },
                                "state": "firing",
                                "value": "1e+00"
                            }
                        ],
                        "annotations": {
                            "message": "This is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty."
                        },
                        "duration": 0,
                        "evaluationTime": 0.000319506,
                        "health": "ok",
                        "labels": {
                            "severity": "none"
                        },
                        "lastEvaluation": "2020-11-29T15:47:25.521574158Z",
                        "name": "Watchdog",
                        "query": "vector(1)",
                        "state": "firing",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.020213966,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-k8s.rules.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:13.315993592Z",
                "name": "k8s.rules",
                "rules": [
                    {
                        "evaluationTime": 0.001890598,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:47:13.316002207Z",
                        "name": "namespace:container_cpu_usage_seconds_total:sum_rate",
                        "query": "sum by(namespace) (rate(container_cpu_usage_seconds_total{container!=\"POD\",image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"}[5m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.003053782,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:47:13.317898221Z",
                        "name": "node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate",
                        "query": "sum by(cluster, namespace, pod, container) (rate(container_cpu_usage_seconds_total{container!=\"POD\",image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"}[5m])) * on(cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=\"\"}))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.003549695,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:47:13.320956312Z",
                        "name": "node_namespace_pod_container:container_memory_working_set_bytes",
                        "query": "container_memory_working_set_bytes{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on(namespace, pod) group_left(node) topk by(namespace, pod) (1, max by(namespace, pod, node) (kube_pod_info{node!=\"\"}))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.003309921,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:47:13.324510366Z",
                        "name": "node_namespace_pod_container:container_memory_rss",
                        "query": "container_memory_rss{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on(namespace, pod) group_left(node) topk by(namespace, pod) (1, max by(namespace, pod, node) (kube_pod_info{node!=\"\"}))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.0023276,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:47:13.327823931Z",
                        "name": "node_namespace_pod_container:container_memory_cache",
                        "query": "container_memory_cache{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on(namespace, pod) group_left(node) topk by(namespace, pod) (1, max by(namespace, pod, node) (kube_pod_info{node!=\"\"}))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.002280342,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:47:13.330154575Z",
                        "name": "node_namespace_pod_container:container_memory_swap",
                        "query": "container_memory_swap{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on(namespace, pod) group_left(node) topk by(namespace, pod) (1, max by(namespace, pod, node) (kube_pod_info{node!=\"\"}))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000908082,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:47:13.332438606Z",
                        "name": "namespace:container_memory_usage_bytes:sum",
                        "query": "sum by(namespace) (container_memory_usage_bytes{container!=\"POD\",image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"})",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000970911,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:47:13.333348554Z",
                        "name": "namespace:kube_pod_container_resource_requests_memory_bytes:sum",
                        "query": "sum by(namespace) (sum by(namespace, pod) (max by(namespace, pod, container) (kube_pod_container_resource_requests_memory_bytes{job=\"kube-state-metrics\"}) * on(namespace, pod) group_left() max by(namespace, pod) (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000832155,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:47:13.334321437Z",
                        "name": "namespace:kube_pod_container_resource_requests_cpu_cores:sum",
                        "query": "sum by(namespace) (sum by(namespace, pod) (max by(namespace, pod, container) (kube_pod_container_resource_requests_cpu_cores{job=\"kube-state-metrics\"}) * on(namespace, pod) group_left() max by(namespace, pod) (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.00066739,
                        "health": "ok",
                        "labels": {
                            "workload_type": "deployment"
                        },
                        "lastEvaluation": "2020-11-29T15:47:13.335155053Z",
                        "name": "namespace_workload_pod:kube_pod_owner:relabel",
                        "query": "max by(cluster, namespace, workload, pod) (label_replace(label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"ReplicaSet\"}, \"replicaset\", \"$1\", \"owner_name\", \"(.*)\") * on(replicaset, namespace) group_left(owner_name) topk by(replicaset, namespace) (1, max by(replicaset, namespace, owner_name) (kube_replicaset_owner{job=\"kube-state-metrics\"})), \"workload\", \"$1\", \"owner_name\", \"(.*)\"))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000229973,
                        "health": "ok",
                        "labels": {
                            "workload_type": "daemonset"
                        },
                        "lastEvaluation": "2020-11-29T15:47:13.335823932Z",
                        "name": "namespace_workload_pod:kube_pod_owner:relabel",
                        "query": "max by(cluster, namespace, workload, pod) (label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"DaemonSet\"}, \"workload\", \"$1\", \"owner_name\", \"(.*)\"))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000150311,
                        "health": "ok",
                        "labels": {
                            "workload_type": "statefulset"
                        },
                        "lastEvaluation": "2020-11-29T15:47:13.336054837Z",
                        "name": "namespace_workload_pod:kube_pod_owner:relabel",
                        "query": "max by(cluster, namespace, workload, pod) (label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"StatefulSet\"}, \"workload\", \"$1\", \"owner_name\", \"(.*)\"))",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.837443287,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kube-apiserver-availability.rules.yaml",
                "interval": 180,
                "lastEvaluation": "2020-11-29T15:45:48.620159765Z",
                "name": "kube-apiserver-availability.rules",
                "rules": [
                    {
                        "evaluationTime": 0.292412948,
                        "health": "ok",
                        "labels": {
                            "verb": "all"
                        },
                        "lastEvaluation": "2020-11-29T15:45:48.620169009Z",
                        "name": "apiserver_request:availability30d",
                        "query": "1 - ((sum(increase(apiserver_request_duration_seconds_count{verb=~\"POST|PUT|PATCH|DELETE\"}[30d])) - sum(increase(apiserver_request_duration_seconds_bucket{le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[30d]))) + (sum(increase(apiserver_request_duration_seconds_count{verb=~\"LIST|GET\"}[30d])) - ((sum(increase(apiserver_request_duration_seconds_bucket{le=\"0.1\",scope=~\"resource|\",verb=~\"LIST|GET\"}[30d])) or vector(0)) + sum(increase(apiserver_request_duration_seconds_bucket{le=\"0.5\",scope=\"namespace\",verb=~\"LIST|GET\"}[30d])) + sum(increase(apiserver_request_duration_seconds_bucket{le=\"5\",scope=\"cluster\",verb=~\"LIST|GET\"}[30d])))) + sum(code:apiserver_request_total:increase30d{code=~\"5..\"} or vector(0))) / sum(code:apiserver_request_total:increase30d)",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.179556774,
                        "health": "ok",
                        "labels": {
                            "verb": "read"
                        },
                        "lastEvaluation": "2020-11-29T15:45:48.912588575Z",
                        "name": "apiserver_request:availability30d",
                        "query": "1 - (sum(increase(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"LIST|GET\"}[30d])) - ((sum(increase(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.1\",scope=~\"resource|\",verb=~\"LIST|GET\"}[30d])) or vector(0)) + sum(increase(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.5\",scope=\"namespace\",verb=~\"LIST|GET\"}[30d])) + sum(increase(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"cluster\",verb=~\"LIST|GET\"}[30d]))) + sum(code:apiserver_request_total:increase30d{code=~\"5..\",verb=\"read\"} or vector(0))) / sum(code:apiserver_request_total:increase30d{verb=\"read\"})",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.114412112,
                        "health": "ok",
                        "labels": {
                            "verb": "write"
                        },
                        "lastEvaluation": "2020-11-29T15:45:49.092151988Z",
                        "name": "apiserver_request:availability30d",
                        "query": "1 - ((sum(increase(apiserver_request_duration_seconds_count{verb=~\"POST|PUT|PATCH|DELETE\"}[30d])) - sum(increase(apiserver_request_duration_seconds_bucket{le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[30d]))) + sum(code:apiserver_request_total:increase30d{code=~\"5..\",verb=\"write\"} or vector(0))) / sum(code:apiserver_request_total:increase30d{verb=\"write\"})",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.110885461,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.206570078Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"2..\",job=\"apiserver\",verb=\"LIST\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.028929069,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.317461819Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"2..\",job=\"apiserver\",verb=\"GET\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.026896567,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.346396736Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"2..\",job=\"apiserver\",verb=\"POST\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.021674998,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.373299861Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"2..\",job=\"apiserver\",verb=\"PUT\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.005670939,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.394980725Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"2..\",job=\"apiserver\",verb=\"PATCH\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.004791436,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.400655786Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"2..\",job=\"apiserver\",verb=\"DELETE\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.00036595,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.405449618Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"3..\",job=\"apiserver\",verb=\"LIST\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000178652,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.405817101Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"3..\",job=\"apiserver\",verb=\"GET\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000172825,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.405996477Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"3..\",job=\"apiserver\",verb=\"POST\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000278588,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.406170185Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"3..\",job=\"apiserver\",verb=\"PUT\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000318487,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.406450834Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"3..\",job=\"apiserver\",verb=\"PATCH\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000230838,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.406770891Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"3..\",job=\"apiserver\",verb=\"DELETE\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000322522,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.407003335Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"4..\",job=\"apiserver\",verb=\"LIST\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.027625709,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.407327366Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"4..\",job=\"apiserver\",verb=\"GET\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.005309891,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.434959152Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"4..\",job=\"apiserver\",verb=\"POST\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.005150186,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.440272881Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"4..\",job=\"apiserver\",verb=\"PUT\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.001000073,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.445426543Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"4..\",job=\"apiserver\",verb=\"PATCH\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.001443733,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.446428945Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"4..\",job=\"apiserver\",verb=\"DELETE\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000950028,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.447874955Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=\"LIST\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.002953566,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.448827251Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=\"GET\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.001427109,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.451782949Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=\"POST\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.002005945,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.453212166Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=\"PUT\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.00090193,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.45522065Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=\"PATCH\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000919794,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:45:49.456124398Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "sum by(code, verb) (increase(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=\"DELETE\"}[30d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000244398,
                        "health": "ok",
                        "labels": {
                            "verb": "read"
                        },
                        "lastEvaluation": "2020-11-29T15:45:49.45704648Z",
                        "name": "code:apiserver_request_total:increase30d",
                        "query": "sum by(code) (code_verb:apiserver_request_total:increase30d{verb=~\"LIST|GET\"})",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000307372,
                        "health": "ok",
                        "labels": {
                            "verb": "write"
                        },
                        "lastEvaluation": "2020-11-29T15:45:49.45729278Z",
                        "name": "code:apiserver_request_total:increase30d",
                        "query": "sum by(code) (code_verb:apiserver_request_total:increase30d{verb=~\"POST|PUT|PATCH|DELETE\"})",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.000854246,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kube-apiserver-slos.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:02.747724288Z",
                "name": "kube-apiserver-slos",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "The API server is burning too much error budget.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorbudgetburn",
                            "summary": "The API server is burning too much error budget."
                        },
                        "duration": 120,
                        "evaluationTime": 0.000352535,
                        "health": "ok",
                        "labels": {
                            "long": "1h",
                            "severity": "critical",
                            "short": "5m"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.747729426Z",
                        "name": "KubeAPIErrorBudgetBurn",
                        "query": "sum(apiserver_request:burnrate1h) > (14.4 * 0.01) and sum(apiserver_request:burnrate5m) > (14.4 * 0.01)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "The API server is burning too much error budget.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorbudgetburn",
                            "summary": "The API server is burning too much error budget."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000190539,
                        "health": "ok",
                        "labels": {
                            "long": "6h",
                            "severity": "critical",
                            "short": "30m"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.748084031Z",
                        "name": "KubeAPIErrorBudgetBurn",
                        "query": "sum(apiserver_request:burnrate6h) > (6 * 0.01) and sum(apiserver_request:burnrate30m) > (6 * 0.01)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "The API server is burning too much error budget.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorbudgetburn",
                            "summary": "The API server is burning too much error budget."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.000168838,
                        "health": "ok",
                        "labels": {
                            "long": "1d",
                            "severity": "warning",
                            "short": "2h"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.748275786Z",
                        "name": "KubeAPIErrorBudgetBurn",
                        "query": "sum(apiserver_request:burnrate1d) > (3 * 0.01) and sum(apiserver_request:burnrate2h) > (3 * 0.01)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "The API server is burning too much error budget.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorbudgetburn",
                            "summary": "The API server is burning too much error budget."
                        },
                        "duration": 10800,
                        "evaluationTime": 0.000130955,
                        "health": "ok",
                        "labels": {
                            "long": "3d",
                            "severity": "warning",
                            "short": "6h"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.748445401Z",
                        "name": "KubeAPIErrorBudgetBurn",
                        "query": "sum(apiserver_request:burnrate3d) > (1 * 0.01) and sum(apiserver_request:burnrate6h) > (1 * 0.01)",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 1.475077823,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kube-apiserver.rules.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:01.976199014Z",
                "name": "kube-apiserver.rules",
                "rules": [
                    {
                        "evaluationTime": 0.196126534,
                        "health": "ok",
                        "labels": {
                            "verb": "read"
                        },
                        "lastEvaluation": "2020-11-29T15:47:01.976206944Z",
                        "name": "apiserver_request:burnrate1d",
                        "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"LIST|GET\"}[1d])) - ((sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.1\",scope=~\"resource|\",verb=~\"LIST|GET\"}[1d])) or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.5\",scope=\"namespace\",verb=~\"LIST|GET\"}[1d])) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"cluster\",verb=~\"LIST|GET\"}[1d])))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[1d]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[1d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.015260125,
                        "health": "ok",
                        "labels": {
                            "verb": "read"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.172340317Z",
                        "name": "apiserver_request:burnrate1h",
                        "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"LIST|GET\"}[1h])) - ((sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.1\",scope=~\"resource|\",verb=~\"LIST|GET\"}[1h])) or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.5\",scope=\"namespace\",verb=~\"LIST|GET\"}[1h])) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"cluster\",verb=~\"LIST|GET\"}[1h])))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[1h]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[1h]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.029621131,
                        "health": "ok",
                        "labels": {
                            "verb": "read"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.187604613Z",
                        "name": "apiserver_request:burnrate2h",
                        "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"LIST|GET\"}[2h])) - ((sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.1\",scope=~\"resource|\",verb=~\"LIST|GET\"}[2h])) or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.5\",scope=\"namespace\",verb=~\"LIST|GET\"}[2h])) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"cluster\",verb=~\"LIST|GET\"}[2h])))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[2h]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[2h]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.010380399,
                        "health": "ok",
                        "labels": {
                            "verb": "read"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.217230941Z",
                        "name": "apiserver_request:burnrate30m",
                        "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"LIST|GET\"}[30m])) - ((sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.1\",scope=~\"resource|\",verb=~\"LIST|GET\"}[30m])) or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.5\",scope=\"namespace\",verb=~\"LIST|GET\"}[30m])) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"cluster\",verb=~\"LIST|GET\"}[30m])))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[30m]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[30m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.350787133,
                        "health": "ok",
                        "labels": {
                            "verb": "read"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.227625379Z",
                        "name": "apiserver_request:burnrate3d",
                        "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"LIST|GET\"}[3d])) - ((sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.1\",scope=~\"resource|\",verb=~\"LIST|GET\"}[3d])) or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.5\",scope=\"namespace\",verb=~\"LIST|GET\"}[3d])) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"cluster\",verb=~\"LIST|GET\"}[3d])))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[3d]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[3d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.007856694,
                        "health": "ok",
                        "labels": {
                            "verb": "read"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.578418945Z",
                        "name": "apiserver_request:burnrate5m",
                        "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"LIST|GET\"}[5m])) - ((sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.1\",scope=~\"resource|\",verb=~\"LIST|GET\"}[5m])) or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.5\",scope=\"namespace\",verb=~\"LIST|GET\"}[5m])) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"cluster\",verb=~\"LIST|GET\"}[5m])))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[5m]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[5m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.062324829,
                        "health": "ok",
                        "labels": {
                            "verb": "read"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.586280177Z",
                        "name": "apiserver_request:burnrate6h",
                        "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"LIST|GET\"}[6h])) - ((sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.1\",scope=~\"resource|\",verb=~\"LIST|GET\"}[6h])) or vector(0)) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"0.5\",scope=\"namespace\",verb=~\"LIST|GET\"}[6h])) + sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"cluster\",verb=~\"LIST|GET\"}[6h])))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[6h]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[6h]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.094221419,
                        "health": "ok",
                        "labels": {
                            "verb": "write"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.648610676Z",
                        "name": "apiserver_request:burnrate1d",
                        "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1d])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[1d]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1d]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.007941888,
                        "health": "ok",
                        "labels": {
                            "verb": "write"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.742837804Z",
                        "name": "apiserver_request:burnrate1h",
                        "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[1h]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1h]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1h]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.014745795,
                        "health": "ok",
                        "labels": {
                            "verb": "write"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.75078262Z",
                        "name": "apiserver_request:burnrate2h",
                        "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[2h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[2h]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[2h]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[2h]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.004956494,
                        "health": "ok",
                        "labels": {
                            "verb": "write"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.765534894Z",
                        "name": "apiserver_request:burnrate30m",
                        "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[30m])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[30m]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[30m]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[30m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.168227487,
                        "health": "ok",
                        "labels": {
                            "verb": "write"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.770495294Z",
                        "name": "apiserver_request:burnrate3d",
                        "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[3d])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[3d]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[3d]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[3d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.004432759,
                        "health": "ok",
                        "labels": {
                            "verb": "write"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.938729188Z",
                        "name": "apiserver_request:burnrate5m",
                        "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.04513735,
                        "health": "ok",
                        "labels": {
                            "verb": "write"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.943165442Z",
                        "name": "apiserver_request:burnrate6h",
                        "query": "((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[6h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[6h]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[6h]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[6h]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.006499349,
                        "health": "ok",
                        "labels": {
                            "verb": "read"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.988310006Z",
                        "name": "code_resource:apiserver_request_total:rate5m",
                        "query": "sum by(code, resource) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[5m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.003192728,
                        "health": "ok",
                        "labels": {
                            "verb": "write"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.994817219Z",
                        "name": "code_resource:apiserver_request_total:rate5m",
                        "query": "sum by(code, resource) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.078892791,
                        "health": "ok",
                        "labels": {
                            "quantile": "0.99",
                            "verb": "read"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.998015358Z",
                        "name": "cluster_quantile:apiserver_request_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.99, sum by(le, resource) (rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",verb=~\"LIST|GET\"}[5m]))) > 0",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.038162577,
                        "health": "ok",
                        "labels": {
                            "quantile": "0.99",
                            "verb": "write"
                        },
                        "lastEvaluation": "2020-11-29T15:47:03.076916109Z",
                        "name": "cluster_quantile:apiserver_request_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.99, sum by(le, resource) (rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))) > 0",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.006512256,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:47:03.115086354Z",
                        "name": "cluster:apiserver_request_duration_seconds:mean5m",
                        "query": "sum without(instance, pod) (rate(apiserver_request_duration_seconds_sum{subresource!=\"log\",verb!~\"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT\"}[5m])) / sum without(instance, pod) (rate(apiserver_request_duration_seconds_count{subresource!=\"log\",verb!~\"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT\"}[5m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.090414343,
                        "health": "ok",
                        "labels": {
                            "quantile": "0.99"
                        },
                        "lastEvaluation": "2020-11-29T15:47:03.121606334Z",
                        "name": "cluster_quantile:apiserver_request_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.99, sum without(instance, pod) (rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",subresource!=\"log\",verb!~\"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT\"}[5m])))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.14372371,
                        "health": "ok",
                        "labels": {
                            "quantile": "0.9"
                        },
                        "lastEvaluation": "2020-11-29T15:47:03.212029975Z",
                        "name": "cluster_quantile:apiserver_request_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.9, sum without(instance, pod) (rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",subresource!=\"log\",verb!~\"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT\"}[5m])))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.095506749,
                        "health": "ok",
                        "labels": {
                            "quantile": "0.5"
                        },
                        "lastEvaluation": "2020-11-29T15:47:03.355760868Z",
                        "name": "cluster_quantile:apiserver_request_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.5, sum without(instance, pod) (rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",subresource!=\"log\",verb!~\"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT\"}[5m])))",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.000972409,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kube-prometheus-general.rules.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:46:57.649815927Z",
                "name": "kube-prometheus-general.rules",
                "rules": [
                    {
                        "evaluationTime": 0.000657739,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:46:57.649824064Z",
                        "name": "count:up1",
                        "query": "count without(instance, pod, node) (up == 1)",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000300877,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:46:57.650484327Z",
                        "name": "count:up0",
                        "query": "count without(instance, pod, node) (up == 0)",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.002452765,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kube-prometheus-node-recording.rules.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:13.689850226Z",
                "name": "kube-prometheus-node-recording.rules",
                "rules": [
                    {
                        "evaluationTime": 0.000624074,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:47:13.689857902Z",
                        "name": "instance:node_cpu:rate:sum",
                        "query": "sum by(instance) (rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\"}[3m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000304545,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:47:13.690483521Z",
                        "name": "instance:node_network_receive_bytes:rate:sum",
                        "query": "sum by(instance) (rate(node_network_receive_bytes_total[3m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000255535,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:47:13.690789353Z",
                        "name": "instance:node_network_transmit_bytes:rate:sum",
                        "query": "sum by(instance) (rate(node_network_transmit_bytes_total[3m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000640879,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:47:13.691045709Z",
                        "name": "instance:node_cpu:ratio",
                        "query": "sum without(cpu, mode) (rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\"}[5m])) / on(instance) group_left() count by(instance) (sum by(instance, cpu) (node_cpu_seconds_total))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000302742,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:47:13.691687719Z",
                        "name": "cluster:node_cpu:sum_rate5m",
                        "query": "sum(rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\"}[5m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000309094,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:47:13.691991247Z",
                        "name": "cluster:node_cpu:ratio",
                        "query": "cluster:node_cpu_seconds_total:rate5m / count(sum by(instance, cpu) (node_cpu_seconds_total))",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.00321787,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kube-scheduler.rules.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:00.190351781Z",
                "name": "kube-scheduler.rules",
                "rules": [
                    {
                        "evaluationTime": 0.000630185,
                        "health": "ok",
                        "labels": {
                            "quantile": "0.99"
                        },
                        "lastEvaluation": "2020-11-29T15:47:00.190360992Z",
                        "name": "cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.99, sum without(instance, pod) (rate(scheduler_e2e_scheduling_duration_seconds_bucket{job=\"kube-scheduler\"}[5m])))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000330593,
                        "health": "ok",
                        "labels": {
                            "quantile": "0.99"
                        },
                        "lastEvaluation": "2020-11-29T15:47:00.19099313Z",
                        "name": "cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.99, sum without(instance, pod) (rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job=\"kube-scheduler\"}[5m])))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000315982,
                        "health": "ok",
                        "labels": {
                            "quantile": "0.99"
                        },
                        "lastEvaluation": "2020-11-29T15:47:00.191324726Z",
                        "name": "cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.99, sum without(instance, pod) (rate(scheduler_binding_duration_seconds_bucket{job=\"kube-scheduler\"}[5m])))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000398921,
                        "health": "ok",
                        "labels": {
                            "quantile": "0.9"
                        },
                        "lastEvaluation": "2020-11-29T15:47:00.19164196Z",
                        "name": "cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.9, sum without(instance, pod) (rate(scheduler_e2e_scheduling_duration_seconds_bucket{job=\"kube-scheduler\"}[5m])))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000360926,
                        "health": "ok",
                        "labels": {
                            "quantile": "0.9"
                        },
                        "lastEvaluation": "2020-11-29T15:47:00.192042946Z",
                        "name": "cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.9, sum without(instance, pod) (rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job=\"kube-scheduler\"}[5m])))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000335334,
                        "health": "ok",
                        "labels": {
                            "quantile": "0.9"
                        },
                        "lastEvaluation": "2020-11-29T15:47:00.192406336Z",
                        "name": "cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.9, sum without(instance, pod) (rate(scheduler_binding_duration_seconds_bucket{job=\"kube-scheduler\"}[5m])))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000285045,
                        "health": "ok",
                        "labels": {
                            "quantile": "0.5"
                        },
                        "lastEvaluation": "2020-11-29T15:47:00.192742953Z",
                        "name": "cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.5, sum without(instance, pod) (rate(scheduler_e2e_scheduling_duration_seconds_bucket{job=\"kube-scheduler\"}[5m])))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000279581,
                        "health": "ok",
                        "labels": {
                            "quantile": "0.5"
                        },
                        "lastEvaluation": "2020-11-29T15:47:00.193028899Z",
                        "name": "cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.5, sum without(instance, pod) (rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job=\"kube-scheduler\"}[5m])))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000257308,
                        "health": "ok",
                        "labels": {
                            "quantile": "0.5"
                        },
                        "lastEvaluation": "2020-11-29T15:47:00.193309388Z",
                        "name": "cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.5, sum without(instance, pod) (rate(scheduler_binding_duration_seconds_bucket{job=\"kube-scheduler\"}[5m])))",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.000507975,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kube-state-metrics.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:16.305577766Z",
                "name": "kube-state-metrics",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "kube-state-metrics is experiencing errors at an elevated rate in list operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatemetricslisterrors",
                            "summary": "kube-state-metrics is experiencing errors in list operations."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000389801,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:16.305585112Z",
                        "name": "KubeStateMetricsListErrors",
                        "query": "(sum(rate(kube_state_metrics_list_total{job=\"kube-state-metrics\",result=\"error\"}[5m])) / sum(rate(kube_state_metrics_list_total{job=\"kube-state-metrics\"}[5m]))) > 0.01",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "kube-state-metrics is experiencing errors at an elevated rate in watch operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatemetricswatcherrors",
                            "summary": "kube-state-metrics is experiencing errors in watch operations."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000106715,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:16.305976609Z",
                        "name": "KubeStateMetricsWatchErrors",
                        "query": "(sum(rate(kube_state_metrics_watch_total{job=\"kube-state-metrics\",result=\"error\"}[5m])) / sum(rate(kube_state_metrics_watch_total{job=\"kube-state-metrics\"}[5m]))) > 0.01",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.002149187,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kubelet.rules.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:26.101547858Z",
                "name": "kubelet.rules",
                "rules": [
                    {
                        "evaluationTime": 0.000998286,
                        "health": "ok",
                        "labels": {
                            "quantile": "0.99"
                        },
                        "lastEvaluation": "2020-11-29T15:47:26.101575702Z",
                        "name": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.99, sum by(instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) * on(instance) group_left(node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"})",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000629404,
                        "health": "ok",
                        "labels": {
                            "quantile": "0.9"
                        },
                        "lastEvaluation": "2020-11-29T15:47:26.102577902Z",
                        "name": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.9, sum by(instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) * on(instance) group_left(node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"})",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000482904,
                        "health": "ok",
                        "labels": {
                            "quantile": "0.5"
                        },
                        "lastEvaluation": "2020-11-29T15:47:26.103210179Z",
                        "name": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.5, sum by(instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) * on(instance) group_left(node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"})",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.007807046,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kubernetes-apps.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:10.003861284Z",
                "name": "kubernetes-apps",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf \"%.2f\" $value }} times / 5 minutes.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping",
                            "summary": "Pod is crash looping."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000790097,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:10.003875951Z",
                        "name": "KubePodCrashLooping",
                        "query": "rate(kube_pod_container_status_restarts_total{job=\"kube-state-metrics\",namespace=~\".*\"}[5m]) * 60 * 5 > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [
                            {
                                "activeAt": "2020-11-27T17:37:40.003560608Z",
                                "annotations": {
                                    "description": "Pod metalk8s-logging/loki-0 has been in a non-ready state for longer than 15 minutes.",
                                    "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready",
                                    "summary": "Pod has been in a non-ready state for more than 15 minutes."
                                },
                                "labels": {
                                    "alertname": "KubePodNotReady",
                                    "namespace": "metalk8s-logging",
                                    "pod": "loki-0",
                                    "severity": "warning"
                                },
                                "state": "firing",
                                "value": "1e+00"
                            }
                        ],
                        "annotations": {
                            "description": "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready",
                            "summary": "Pod has been in a non-ready state for more than 15 minutes."
                        },
                        "duration": 900,
                        "evaluationTime": 0.001455284,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:10.004668109Z",
                        "name": "KubePodNotReady",
                        "query": "sum by(namespace, pod) (max by(namespace, pod) (kube_pod_status_phase{job=\"kube-state-metrics\",namespace=~\".*\",phase=~\"Pending|Unknown\"}) * on(namespace, pod) group_left(owner_kind) topk by(namespace, pod) (1, max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!=\"Job\"}))) > 0",
                        "state": "firing",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentgenerationmismatch",
                            "summary": "Deployment generation mismatch due to possible roll-back"
                        },
                        "duration": 900,
                        "evaluationTime": 0.000371557,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:10.006125735Z",
                        "name": "KubeDeploymentGenerationMismatch",
                        "query": "kube_deployment_status_observed_generation{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_deployment_metadata_generation{job=\"kube-state-metrics\",namespace=~\".*\"}",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 15 minutes.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch",
                            "summary": "Deployment has not matched the expected number of replicas."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000550333,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:10.006498667Z",
                        "name": "KubeDeploymentReplicasMismatch",
                        "query": "(kube_deployment_spec_replicas{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_deployment_status_replicas_available{job=\"kube-state-metrics\",namespace=~\".*\"}) and (changes(kube_deployment_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\".*\"}[5m]) == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [
                            {
                                "activeAt": "2020-11-27T17:37:40.003560608Z",
                                "annotations": {
                                    "description": "StatefulSet metalk8s-logging/loki has not matched the expected number of replicas for longer than 15 minutes.",
                                    "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetreplicasmismatch",
                                    "summary": "Deployment has not matched the expected number of replicas."
                                },
                                "labels": {
                                    "alertname": "KubeStatefulSetReplicasMismatch",
                                    "container": "kube-state-metrics",
                                    "endpoint": "http",
                                    "instance": "10.233.15.20:8080",
                                    "job": "kube-state-metrics",
                                    "namespace": "metalk8s-logging",
                                    "pod": "prometheus-operator-kube-state-metrics-5dbbc99765-k8snz",
                                    "service": "prometheus-operator-kube-state-metrics",
                                    "severity": "warning",
                                    "statefulset": "loki"
                                },
                                "state": "firing",
                                "value": "0e+00"
                            }
                        ],
                        "annotations": {
                            "description": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetreplicasmismatch",
                            "summary": "Deployment has not matched the expected number of replicas."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000515021,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:10.007050077Z",
                        "name": "KubeStatefulSetReplicasMismatch",
                        "query": "(kube_statefulset_status_replicas_ready{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_statefulset_status_replicas{job=\"kube-state-metrics\",namespace=~\".*\"}) and (changes(kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\".*\"}[5m]) == 0)",
                        "state": "firing",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetgenerationmismatch",
                            "summary": "StatefulSet generation mismatch due to possible roll-back"
                        },
                        "duration": 900,
                        "evaluationTime": 0.000310667,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:10.007566383Z",
                        "name": "KubeStatefulSetGenerationMismatch",
                        "query": "kube_statefulset_status_observed_generation{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_statefulset_metadata_generation{job=\"kube-state-metrics\",namespace=~\".*\"}",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetupdatenotrolledout",
                            "summary": "StatefulSet update has not been rolled out."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000411506,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:10.007878371Z",
                        "name": "KubeStatefulSetUpdateNotRolledOut",
                        "query": "(max without(revision) (kube_statefulset_status_current_revision{job=\"kube-state-metrics\",namespace=~\".*\"} unless kube_statefulset_status_update_revision{job=\"kube-state-metrics\",namespace=~\".*\"}) * (kube_statefulset_replicas{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\".*\"})) and (changes(kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\".*\"}[5m]) == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has not finished or progressed for at least 15 minutes.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetrolloutstuck",
                            "summary": "DaemonSet rollout is stuck."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000863026,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:10.008290839Z",
                        "name": "KubeDaemonSetRolloutStuck",
                        "query": "((kube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"}) or (kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\",namespace=~\".*\"} != 0) or (kube_daemonset_updated_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"}) or (kube_daemonset_status_number_available{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"})) and (changes(kube_daemonset_updated_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"}[5m]) == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Pod {{ $labels.namespace }}/{{ $labels.pod }} container {{ $labels.container}} has been in waiting state for longer than 1 hour.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontainerwaiting",
                            "summary": "Pod container waiting longer than 1 hour"
                        },
                        "duration": 3600,
                        "evaluationTime": 0.001678831,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:10.009155411Z",
                        "name": "KubeContainerWaiting",
                        "query": "sum by(namespace, pod, container) (kube_pod_container_status_waiting_reason{job=\"kube-state-metrics\",namespace=~\".*\"}) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetnotscheduled",
                            "summary": "DaemonSet pods are not scheduled."
                        },
                        "duration": 600,
                        "evaluationTime": 0.00029591,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:10.010836026Z",
                        "name": "KubeDaemonSetNotScheduled",
                        "query": "kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"} - kube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"} > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetmisscheduled",
                            "summary": "DaemonSet pods are misscheduled."
                        },
                        "duration": 900,
                        "evaluationTime": 0.00011597,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:10.011132757Z",
                        "name": "KubeDaemonSetMisScheduled",
                        "query": "kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\",namespace=~\".*\"} > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than 12 hours to complete.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobcompletion",
                            "summary": "Job did not complete in time"
                        },
                        "duration": 43200,
                        "evaluationTime": 9.9577e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:10.011249721Z",
                        "name": "KubeJobCompletion",
                        "query": "kube_job_spec_completions{job=\"kube-state-metrics\",namespace=~\".*\"} - kube_job_status_succeeded{job=\"kube-state-metrics\",namespace=~\".*\"} > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete. Removing failed job after investigation should clear this alert.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobfailed",
                            "summary": "Job failed to complete."
                        },
                        "duration": 900,
                        "evaluationTime": 6.3216e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:10.011350062Z",
                        "name": "KubeJobFailed",
                        "query": "kube_job_failed{job=\"kube-state-metrics\",namespace=~\".*\"} > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "HPA {{ $labels.namespace }}/{{ $labels.hpa }} has not matched the desired number of replicas for longer than 15 minutes.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubehpareplicasmismatch",
                            "summary": "HPA has not matched descired number of replicas."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000143527,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:10.011413792Z",
                        "name": "KubeHpaReplicasMismatch",
                        "query": "(kube_hpa_status_desired_replicas{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_hpa_status_current_replicas{job=\"kube-state-metrics\",namespace=~\".*\"}) and changes(kube_hpa_status_current_replicas[15m]) == 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "HPA {{ $labels.namespace }}/{{ $labels.hpa }} has been running at max replicas for longer than 15 minutes.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubehpamaxedout",
                            "summary": "HPA is running at max replicas"
                        },
                        "duration": 900,
                        "evaluationTime": 0.000108215,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:10.011557892Z",
                        "name": "KubeHpaMaxedOut",
                        "query": "kube_hpa_status_current_replicas{job=\"kube-state-metrics\",namespace=~\".*\"} == kube_hpa_spec_max_replicas{job=\"kube-state-metrics\",namespace=~\".*\"}",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.001852707,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kubernetes-resources.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:02.941727725Z",
                "name": "kubernetes-resources",
                "rules": [
                    {
                        "alerts": [
                            {
                                "activeAt": "2020-11-27T17:38:02.940582493Z",
                                "annotations": {
                                    "description": "Cluster has overcommitted CPU resource requests for Pods and cannot tolerate node failure.",
                                    "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit",
                                    "summary": "Cluster has overcommitted CPU resource requests."
                                },
                                "labels": {
                                    "alertname": "KubeCPUOvercommit",
                                    "severity": "warning"
                                },
                                "state": "firing",
                                "value": "3.5625e-01"
                            }
                        ],
                        "annotations": {
                            "description": "Cluster has overcommitted CPU resource requests for Pods and cannot tolerate node failure.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit",
                            "summary": "Cluster has overcommitted CPU resource requests."
                        },
                        "duration": 300,
                        "evaluationTime": 0.000571666,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.941732069Z",
                        "name": "KubeCPUOvercommit",
                        "query": "sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum) / sum(kube_node_status_allocatable_cpu_cores) > (count(kube_node_status_allocatable_cpu_cores) - 1) / count(kube_node_status_allocatable_cpu_cores)",
                        "state": "firing",
                        "type": "alerting"
                    },
                    {
                        "alerts": [
                            {
                                "activeAt": "2020-11-27T17:38:02.940582493Z",
                                "annotations": {
                                    "description": "Cluster has overcommitted memory resource requests for Pods and cannot tolerate node failure.",
                                    "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememoryovercommit",
                                    "summary": "Cluster has overcommitted memory resource requests."
                                },
                                "labels": {
                                    "alertname": "KubeMemoryOvercommit",
                                    "severity": "warning"
                                },
                                "state": "firing",
                                "value": "7.648251029691439e-02"
                            }
                        ],
                        "annotations": {
                            "description": "Cluster has overcommitted memory resource requests for Pods and cannot tolerate node failure.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememoryovercommit",
                            "summary": "Cluster has overcommitted memory resource requests."
                        },
                        "duration": 300,
                        "evaluationTime": 0.000441585,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.942305804Z",
                        "name": "KubeMemoryOvercommit",
                        "query": "sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum) / sum(kube_node_status_allocatable_memory_bytes) > (count(kube_node_status_allocatable_memory_bytes) - 1) / count(kube_node_status_allocatable_memory_bytes)",
                        "state": "firing",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Cluster has overcommitted CPU resource requests for Namespaces.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuquotaovercommit",
                            "summary": "Cluster has overcommitted CPU resource requests."
                        },
                        "duration": 300,
                        "evaluationTime": 0.000137319,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.942748874Z",
                        "name": "KubeCPUQuotaOvercommit",
                        "query": "sum(kube_resourcequota{job=\"kube-state-metrics\",resource=\"cpu\",type=\"hard\"}) / sum(kube_node_status_allocatable_cpu_cores) > 1.5",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Cluster has overcommitted memory resource requests for Namespaces.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememoryquotaovercommit",
                            "summary": "Cluster has overcommitted memory resource requests."
                        },
                        "duration": 300,
                        "evaluationTime": 9.1605e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.94288702Z",
                        "name": "KubeMemoryQuotaOvercommit",
                        "query": "sum(kube_resourcequota{job=\"kube-state-metrics\",resource=\"memory\",type=\"hard\"}) / sum(kube_node_status_allocatable_memory_bytes{job=\"node-exporter\"}) > 1.5",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotaalmostfull",
                            "summary": "Namespace quota is going to be full."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000172787,
                        "health": "ok",
                        "labels": {
                            "severity": "info"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.942979345Z",
                        "name": "KubeQuotaAlmostFull",
                        "query": "kube_resourcequota{job=\"kube-state-metrics\",type=\"used\"} / ignoring(instance, job, type) (kube_resourcequota{job=\"kube-state-metrics\",type=\"hard\"} > 0) > 0.9 < 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotafullyused",
                            "summary": "Namespace quota is fully used."
                        },
                        "duration": 900,
                        "evaluationTime": 8.9996e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "info"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.943152966Z",
                        "name": "KubeQuotaFullyUsed",
                        "query": "kube_resourcequota{job=\"kube-state-metrics\",type=\"used\"} / ignoring(instance, job, type) (kube_resourcequota{job=\"kube-state-metrics\",type=\"hard\"} > 0) == 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotaexceeded",
                            "summary": "Namespace quota has exceeded the limits."
                        },
                        "duration": 900,
                        "evaluationTime": 8.076e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.943243607Z",
                        "name": "KubeQuotaExceeded",
                        "query": "kube_resourcequota{job=\"kube-state-metrics\",type=\"used\"} / ignoring(instance, job, type) (kube_resourcequota{job=\"kube-state-metrics\",type=\"hard\"} > 0) > 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }}.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-cputhrottlinghigh",
                            "summary": "Processes experience elevated CPU throttling."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000253342,
                        "health": "ok",
                        "labels": {
                            "severity": "info"
                        },
                        "lastEvaluation": "2020-11-29T15:47:02.943324883Z",
                        "name": "CPUThrottlingHigh",
                        "query": "sum by(container, pod, namespace) (increase(container_cpu_cfs_throttled_periods_total{container!=\"\"}[5m])) / sum by(container, pod, namespace) (increase(container_cpu_cfs_periods_total[5m])) > (25 / 100)",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.001519247,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kubernetes-storage.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:26.331092791Z",
                "name": "kubernetes-storage",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefillingup",
                            "summary": "PersistentVolume is filling up."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000555733,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:26.331102543Z",
                        "name": "KubePersistentVolumeFillingUp",
                        "query": "kubelet_volume_stats_available_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"} < 0.03",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefillingup",
                            "summary": "PersistentVolume is filling up."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.000783937,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:26.33166029Z",
                        "name": "KubePersistentVolumeFillingUp",
                        "query": "(kubelet_volume_stats_available_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"}) < 0.15 and predict_linear(kubelet_volume_stats_available_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"}[6h], 4 * 24 * 3600) < 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase }}.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeerrors",
                            "summary": "PersistentVolume is having issues with provisioning."
                        },
                        "duration": 300,
                        "evaluationTime": 0.00015717,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:26.332445826Z",
                        "name": "KubePersistentVolumeErrors",
                        "query": "kube_persistentvolume_status_phase{job=\"kube-state-metrics\",phase=~\"Failed|Pending\"} > 0",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.001859983,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kubernetes-system-apiserver.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:22.146791411Z",
                "name": "kubernetes-system-apiserver",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "A client certificate used to authenticate to the apiserver is expiring in less than 7.0 days.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration",
                            "summary": "Client certificate is about to expire."
                        },
                        "duration": 0,
                        "evaluationTime": 0.000702848,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:22.146799946Z",
                        "name": "KubeClientCertificateExpiration",
                        "query": "apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} > 0 and on(job) histogram_quantile(0.01, sum by(job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) < 604800",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration",
                            "summary": "Client certificate is about to expire."
                        },
                        "duration": 0,
                        "evaluationTime": 0.00039052,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:22.147504838Z",
                        "name": "KubeClientCertificateExpiration",
                        "query": "apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} > 0 and on(job) histogram_quantile(0.01, sum by(job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) < 86400",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "An aggregated API {{ $labels.name }}/{{ $labels.namespace }} has reported errors. The number of errors have increased for it in the past five minutes. High values indicate that the availability of the service changes too often.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-aggregatedapierrors",
                            "summary": "An aggregated API has reported errors."
                        },
                        "duration": 0,
                        "evaluationTime": 0.000122533,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:22.147896597Z",
                        "name": "AggregatedAPIErrors",
                        "query": "sum by(name, namespace) (increase(aggregator_unavailable_apiservice_count[5m])) > 2",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "An aggregated API {{ $labels.name }}/{{ $labels.namespace }} has been only {{ $value | humanize }}% available over the last 10m.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-aggregatedapidown",
                            "summary": "An aggregated API is down."
                        },
                        "duration": 300,
                        "evaluationTime": 0.000532759,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:22.148019781Z",
                        "name": "AggregatedAPIDown",
                        "query": "(1 - max by(name, namespace) (avg_over_time(aggregator_unavailable_apiservice[10m]))) * 100 < 85",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "KubeAPI has disappeared from Prometheus target discovery.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapidown",
                            "summary": "Target disappeared from Prometheus target discovery."
                        },
                        "duration": 900,
                        "evaluationTime": 9.5328e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:22.148553774Z",
                        "name": "KubeAPIDown",
                        "query": "absent(up{job=\"apiserver\"} == 1)",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.000490348,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kubernetes-system-controller-manager.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:05.675350081Z",
                "name": "kubernetes-system-controller-manager",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "KubeControllerManager has disappeared from Prometheus target discovery.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontrollermanagerdown",
                            "summary": "Target disappeared from Prometheus target discovery."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000475775,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:05.675358552Z",
                        "name": "KubeControllerManagerDown",
                        "query": "absent(up{job=\"kube-controller-manager\"} == 1)",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.002882228,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kubernetes-system-kubelet.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:08.336398439Z",
                "name": "kubernetes-system-kubelet",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $labels.node }} has been unready for more than 15 minutes.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodenotready",
                            "summary": "Node is not ready."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000329282,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:08.336406423Z",
                        "name": "KubeNodeNotReady",
                        "query": "kube_node_status_condition{condition=\"Ready\",job=\"kube-state-metrics\",status=\"true\"} == 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $labels.node }} is unreachable and some workloads may be rescheduled.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodeunreachable",
                            "summary": "Node is unreachable."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000276028,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:08.336737345Z",
                        "name": "KubeNodeUnreachable",
                        "query": "(kube_node_spec_taint{effect=\"NoSchedule\",job=\"kube-state-metrics\",key=\"node.kubernetes.io/unreachable\"} unless ignoring(key, value) kube_node_spec_taint{job=\"kube-state-metrics\",key=~\"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn\"}) == 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage }} of its Pod capacity.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubelettoomanypods",
                            "summary": "Kubelet is running at capacity."
                        },
                        "duration": 900,
                        "evaluationTime": 0.001065668,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:08.337014222Z",
                        "name": "KubeletTooManyPods",
                        "query": "count by(node) ((kube_pod_status_phase{job=\"kube-state-metrics\",phase=\"Running\"} == 1) * on(instance, pod, namespace, cluster) group_left(node) topk by(instance, pod, namespace, cluster) (1, kube_pod_info{job=\"kube-state-metrics\"})) / max by(node) (kube_node_status_capacity_pods{job=\"kube-state-metrics\"} != 1) > 0.95",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "The readiness status of node {{ $labels.node }} has changed {{ $value }} times in the last 15 minutes.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodereadinessflapping",
                            "summary": "Node readiness status is flapping."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000137282,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:08.338081956Z",
                        "name": "KubeNodeReadinessFlapping",
                        "query": "sum by(node) (changes(kube_node_status_condition{condition=\"Ready\",status=\"true\"}[15m])) > 2",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of {{ $value }} seconds on node {{ $labels.node }}.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletplegdurationhigh",
                            "summary": "Kubelet Pod Lifecycle Event Generator is taking too long to relist."
                        },
                        "duration": 300,
                        "evaluationTime": 8.4394e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:08.338219934Z",
                        "name": "KubeletPlegDurationHigh",
                        "query": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile=\"0.99\"} >= 10",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Kubelet Pod startup 99th percentile latency is {{ $value }} seconds on node {{ $labels.node }}.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletpodstartuplatencyhigh",
                            "summary": "Kubelet Pod startup latency is too high."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000448923,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:08.338305324Z",
                        "name": "KubeletPodStartUpLatencyHigh",
                        "query": "histogram_quantile(0.99, sum by(instance, le) (rate(kubelet_pod_worker_duration_seconds_bucket{job=\"kubelet\",metrics_path=\"/metrics\"}[5m]))) * on(instance) group_left(node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"} > 60",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Client certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletclientcertificateexpiration",
                            "summary": "Kubelet client certificate is about to expire."
                        },
                        "duration": 0,
                        "evaluationTime": 6.2771e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:08.338755847Z",
                        "name": "KubeletClientCertificateExpiration",
                        "query": "kubelet_certificate_manager_client_ttl_seconds < 604800",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Client certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletclientcertificateexpiration",
                            "summary": "Kubelet client certificate is about to expire."
                        },
                        "duration": 0,
                        "evaluationTime": 3.6178e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:08.338819209Z",
                        "name": "KubeletClientCertificateExpiration",
                        "query": "kubelet_certificate_manager_client_ttl_seconds < 86400",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Server certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletservercertificateexpiration",
                            "summary": "Kubelet server certificate is about to expire."
                        },
                        "duration": 0,
                        "evaluationTime": 3.5656e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:08.338856107Z",
                        "name": "KubeletServerCertificateExpiration",
                        "query": "kubelet_certificate_manager_server_ttl_seconds < 604800",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Server certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletservercertificateexpiration",
                            "summary": "Kubelet server certificate is about to expire."
                        },
                        "duration": 0,
                        "evaluationTime": 3.6583e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:08.338892308Z",
                        "name": "KubeletServerCertificateExpiration",
                        "query": "kubelet_certificate_manager_server_ttl_seconds < 86400",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Kubelet on node {{ $labels.node }} has failed to renew its client certificate ({{ $value | humanize }} errors in the last 5 minutes).",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletclientcertificaterenewalerrors",
                            "summary": "Kubelet has failed to renew its client certificate."
                        },
                        "duration": 900,
                        "evaluationTime": 7.54e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:08.338929457Z",
                        "name": "KubeletClientCertificateRenewalErrors",
                        "query": "increase(kubelet_certificate_manager_client_expiration_renew_errors[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Kubelet on node {{ $labels.node }} has failed to renew its server certificate ({{ $value | humanize }} errors in the last 5 minutes).",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletservercertificaterenewalerrors",
                            "summary": "Kubelet has failed to renew its server certificate."
                        },
                        "duration": 900,
                        "evaluationTime": 5.3513e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:08.339005407Z",
                        "name": "KubeletServerCertificateRenewalErrors",
                        "query": "increase(kubelet_server_expiration_renew_errors[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Kubelet has disappeared from Prometheus target discovery.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletdown",
                            "summary": "Target disappeared from Prometheus target discovery."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000214561,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:08.339059449Z",
                        "name": "KubeletDown",
                        "query": "absent(up{job=\"kubelet\",metrics_path=\"/metrics\"} == 1)",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.000418894,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kubernetes-system-scheduler.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:19.588006843Z",
                "name": "kubernetes-system-scheduler",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "KubeScheduler has disappeared from Prometheus target discovery.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeschedulerdown",
                            "summary": "Target disappeared from Prometheus target discovery."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000404185,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:19.588017045Z",
                        "name": "KubeSchedulerDown",
                        "query": "absent(up{job=\"kube-scheduler\"} == 1)",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.00117742,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kubernetes-system.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:17.922684345Z",
                "name": "kubernetes-system",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "There are {{ $value }} different semantic versions of Kubernetes components running.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeversionmismatch",
                            "summary": "Different semantic versions of Kubernetes components running."
                        },
                        "duration": 900,
                        "evaluationTime": 0.00051567,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:17.922692589Z",
                        "name": "KubeVersionMismatch",
                        "query": "count(count by(gitVersion) (label_replace(kubernetes_build_info{job!~\"kube-dns|coredns\"}, \"gitVersion\", \"$1\", \"gitVersion\", \"(v[0-9]*.[0-9]*).*\"))) > 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ $value | humanizePercentage }} errors.'",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors",
                            "summary": "Kubernetes API server client is experiencing errors."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000647946,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:17.923210239Z",
                        "name": "KubeClientErrors",
                        "query": "(sum by(instance, job) (rate(rest_client_requests_total{code=~\"5..\"}[5m])) / sum by(instance, job) (rate(rest_client_requests_total[5m]))) > 0.01",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.008929305,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-node-exporter.rules.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:46:57.32455259Z",
                "name": "node-exporter.rules",
                "rules": [
                    {
                        "evaluationTime": 0.00115722,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:46:57.324559707Z",
                        "name": "instance:node_num_cpu:sum",
                        "query": "count without(cpu) (count without(mode) (node_cpu_seconds_total{job=\"node-exporter\"}))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000380954,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:46:57.325721166Z",
                        "name": "instance:node_cpu_utilisation:rate1m",
                        "query": "1 - avg without(cpu, mode) (rate(node_cpu_seconds_total{job=\"node-exporter\",mode=\"idle\"}[1m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000284219,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:46:57.326104519Z",
                        "name": "instance:node_load1_per_cpu:ratio",
                        "query": "(node_load1{job=\"node-exporter\"} / instance:node_num_cpu:sum{job=\"node-exporter\"})",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.00537882,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:46:57.326390441Z",
                        "name": "instance:node_memory_utilisation:ratio",
                        "query": "1 - (node_memory_MemAvailable_bytes{job=\"node-exporter\"} / node_memory_MemTotal_bytes{job=\"node-exporter\"})",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000197513,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:46:57.331775195Z",
                        "name": "instance:node_vmstat_pgmajfault:rate1m",
                        "query": "rate(node_vmstat_pgmajfault{job=\"node-exporter\"}[1m])",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000214359,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:46:57.331973892Z",
                        "name": "instance_device:node_disk_io_time_seconds:rate1m",
                        "query": "rate(node_disk_io_time_seconds_total{device=~\"mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+\",job=\"node-exporter\"}[1m])",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000167388,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:46:57.332189183Z",
                        "name": "instance_device:node_disk_io_time_weighted_seconds:rate1m",
                        "query": "rate(node_disk_io_time_weighted_seconds_total{device=~\"mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+\",job=\"node-exporter\"}[1m])",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000288957,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:46:57.332358158Z",
                        "name": "instance:node_network_receive_bytes_excluding_lo:rate1m",
                        "query": "sum without(device) (rate(node_network_receive_bytes_total{device!=\"lo\",job=\"node-exporter\"}[1m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000335918,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:46:57.332648127Z",
                        "name": "instance:node_network_transmit_bytes_excluding_lo:rate1m",
                        "query": "sum without(device) (rate(node_network_transmit_bytes_total{device!=\"lo\",job=\"node-exporter\"}[1m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000268072,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:46:57.332985155Z",
                        "name": "instance:node_network_receive_drop_excluding_lo:rate1m",
                        "query": "sum without(device) (rate(node_network_receive_drop_total{device!=\"lo\",job=\"node-exporter\"}[1m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000224773,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:46:57.333254175Z",
                        "name": "instance:node_network_transmit_drop_excluding_lo:rate1m",
                        "query": "sum without(device) (rate(node_network_transmit_drop_total{device!=\"lo\",job=\"node-exporter\"}[1m]))",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.028106234,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-node-exporter.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:05.358857596Z",
                "name": "node-exporter",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemspacefillingup",
                            "summary": "Filesystem is predicted to run out of space within the next 24 hours."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.0056904,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:05.358870831Z",
                        "name": "NodeFilesystemSpaceFillingUp",
                        "query": "(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 < 40 and predict_linear(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"}[6h], 24 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up fast.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemspacefillingup",
                            "summary": "Filesystem is predicted to run out of space within the next 4 hours."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.005197011,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:05.364565763Z",
                        "name": "NodeFilesystemSpaceFillingUp",
                        "query": "(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 < 20 and predict_linear(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"}[6h], 4 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutofspace",
                            "summary": "Filesystem has less than 5% space left."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.001343539,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:05.369766881Z",
                        "name": "NodeFilesystemAlmostOutOfSpace",
                        "query": "(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 < 5 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutofspace",
                            "summary": "Filesystem has less than 3% space left."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.001053215,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:05.371112679Z",
                        "name": "NodeFilesystemAlmostOutOfSpace",
                        "query": "(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 < 3 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemfilesfillingup",
                            "summary": "Filesystem is predicted to run out of inodes within the next 24 hours."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.005565903,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:05.372167458Z",
                        "name": "NodeFilesystemFilesFillingUp",
                        "query": "(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\"} * 100 < 40 and predict_linear(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"}[6h], 24 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up fast.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemfilesfillingup",
                            "summary": "Filesystem is predicted to run out of inodes within the next 4 hours."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.005196726,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:05.377735196Z",
                        "name": "NodeFilesystemFilesFillingUp",
                        "query": "(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\"} * 100 < 20 and predict_linear(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"}[6h], 4 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutoffiles",
                            "summary": "Filesystem has less than 5% inodes left."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.001289453,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:05.382934053Z",
                        "name": "NodeFilesystemAlmostOutOfFiles",
                        "query": "(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\"} * 100 < 5 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutoffiles",
                            "summary": "Filesystem has less than 3% inodes left."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.001153154,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:05.384226152Z",
                        "name": "NodeFilesystemAlmostOutOfFiles",
                        "query": "(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\"} * 100 < 3 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last two minutes.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodenetworkreceiveerrs",
                            "summary": "Network interface is reporting many receive errors."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.000308511,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:05.385380895Z",
                        "name": "NodeNetworkReceiveErrs",
                        "query": "increase(node_network_receive_errs_total[2m]) > 10",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last two minutes.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodenetworktransmiterrs",
                            "summary": "Network interface is reporting many transmit errors."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.000345897,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:05.385690623Z",
                        "name": "NodeNetworkTransmitErrs",
                        "query": "increase(node_network_transmit_errs_total[2m]) > 10",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $value | humanizePercentage }} of conntrack entries are used",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodehighnumberconntrackentriesused",
                            "summary": "Number of conntrack are getting close to the limit"
                        },
                        "duration": 0,
                        "evaluationTime": 0.000172827,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:05.386037883Z",
                        "name": "NodeHighNumberConntrackEntriesUsed",
                        "query": "(node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "Clock on {{ $labels.instance }} is out of sync by more than 300s. Ensure NTP is configured correctly on this host.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodeclockskewdetected",
                            "summary": "Clock skew detected."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000282548,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:05.3862123Z",
                        "name": "NodeClockSkewDetected",
                        "query": "(node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0) or (node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodeclocknotsynchronising",
                            "summary": "Clock not synchronising."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000189158,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:05.38649624Z",
                        "name": "NodeClockNotSynchronising",
                        "query": "min_over_time(node_timex_sync_status[5m]) == 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Node Exporter text file collector failed to scrape.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodetextfilecollectorscrapeerror",
                            "summary": "Node Exporter text file collector failed to scrape."
                        },
                        "duration": 0,
                        "evaluationTime": 0.000125434,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:05.386688143Z",
                        "name": "NodeTextFileCollectorScrapeError",
                        "query": "node_textfile_scrape_error{job=\"node-exporter\"} == 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "RAID array '{{ $labels.device }}' on {{ $labels.instance }} is in degraded state due to 1 or more disks failures. Number of spare drives is insufficient to fix issue automatically.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-noderaiddegraded",
                            "summary": "RAID Array is degraded"
                        },
                        "duration": 900,
                        "evaluationTime": 9.4503e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:05.386814361Z",
                        "name": "NodeRAIDDegraded",
                        "query": "node_md_disks_required - ignoring(state) (node_md_disks{state=\"active\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "At least 1 device in RAID array on {{ $labels.instance }} failed. Array '{{ $labels.device }}' needs attention and possibly a disk swap.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-noderaiddiskfailure",
                            "summary": "Failed device in RAID array"
                        },
                        "duration": 0,
                        "evaluationTime": 5.1759e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:05.386909532Z",
                        "name": "NodeRAIDDiskFailure",
                        "query": "node_md_disks{state=\"fail\"} >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.000565604,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-node-network.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:16.986485628Z",
                "name": "node-network",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "Network interface \"{{ $labels.device }}\" changing it's up status often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}\""
                        },
                        "duration": 120,
                        "evaluationTime": 0.000553822,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:16.986493549Z",
                        "name": "NodeNetworkInterfaceFlapping",
                        "query": "changes(node_network_up{device!~\"veth.+\",job=\"node-exporter\"}[2m]) > 2",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.002635622,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-node.rules.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:18.10688951Z",
                "name": "node.rules",
                "rules": [
                    {
                        "evaluationTime": 0.000539122,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:47:18.10689763Z",
                        "name": ":kube_pod_info_node_count:",
                        "query": "sum(min by(cluster, node) (kube_pod_info{node!=\"\"}))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000938064,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:47:18.107438679Z",
                        "name": "node_namespace_pod:kube_pod_info:",
                        "query": "topk by(namespace, pod) (1, max by(node, namespace, pod) (label_replace(kube_pod_info{job=\"kube-state-metrics\",node!=\"\"}, \"pod\", \"$1\", \"pod\", \"(.*)\")))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000849944,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:47:18.10837963Z",
                        "name": "node:node_num_cpu:sum",
                        "query": "count by(cluster, node) (sum by(node, cpu) (node_cpu_seconds_total{job=\"node-exporter\"} * on(namespace, pod) group_left(node) node_namespace_pod:kube_pod_info:))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000290351,
                        "health": "ok",
                        "lastEvaluation": "2020-11-29T15:47:18.109231218Z",
                        "name": ":node_memory_MemAvailable_bytes:sum",
                        "query": "sum by(cluster) (node_memory_MemAvailable_bytes{job=\"node-exporter\"} or (node_memory_Buffers_bytes{job=\"node-exporter\"} + node_memory_Cached_bytes{job=\"node-exporter\"} + node_memory_MemFree_bytes{job=\"node-exporter\"} + node_memory_Slab_bytes{job=\"node-exporter\"}))",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.001687445,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-prometheus-operator.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:16.673229638Z",
                "name": "prometheus-operator",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Errors while performing List operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-prometheusoperatorlisterrors",
                            "summary": "Errors while performing list operations in controller."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000681243,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:16.673237695Z",
                        "name": "PrometheusOperatorListErrors",
                        "query": "(sum by(controller, namespace) (rate(prometheus_operator_list_operations_failed_total{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\"}[10m])) / sum by(controller, namespace) (rate(prometheus_operator_list_operations_total{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\"}[10m]))) > 0.4",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Errors while performing watch operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-prometheusoperatorwatcherrors",
                            "summary": "Errors while performing watch operations in controller."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000266594,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:16.673921278Z",
                        "name": "PrometheusOperatorWatchErrors",
                        "query": "(sum by(controller, namespace) (rate(prometheus_operator_watch_operations_failed_total{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\"}[10m])) / sum by(controller, namespace) (rate(prometheus_operator_watch_operations_total{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\"}[10m]))) > 0.4",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Controller {{ $labels.controller }} in {{ $labels.namespace }} namespace fails to reconcile {{ $value }} objects.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-prometheusoperatorsyncfailed",
                            "summary": "Last controller reconciliation failed"
                        },
                        "duration": 600,
                        "evaluationTime": 0.000107201,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:16.674188695Z",
                        "name": "PrometheusOperatorSyncFailed",
                        "query": "min_over_time(prometheus_operator_syncs{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\",status=\"failed\"}[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $value | humanizePercentage }} of reconciling operations failed for {{ $labels.controller }} controller in {{ $labels.namespace }} namespace.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-prometheusoperatorreconcileerrors",
                            "summary": "Errors while reconciling controller."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000208131,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:16.674296567Z",
                        "name": "PrometheusOperatorReconcileErrors",
                        "query": "(sum by(controller, namespace) (rate(prometheus_operator_reconcile_errors_total{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\"}[5m]))) / (sum by(controller, namespace) (rate(prometheus_operator_reconcile_operations_total{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\"}[5m]))) > 0.1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-prometheusoperatornodelookuperrors",
                            "summary": "Errors while reconciling Prometheus."
                        },
                        "duration": 600,
                        "evaluationTime": 7.5549e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:16.674505344Z",
                        "name": "PrometheusOperatorNodeLookupErrors",
                        "query": "rate(prometheus_operator_node_address_lookup_errors_total{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\"}[5m]) > 0.1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus operator in {{ $labels.namespace }} namespace isn't ready to reconcile {{ $labels.controller }} resources.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-prometheusoperatornotready",
                            "summary": "Prometheus operator not ready"
                        },
                        "duration": 300,
                        "evaluationTime": 0.000115494,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:16.674581474Z",
                        "name": "PrometheusOperatorNotReady",
                        "query": "min by(namespace, controller) (max_over_time(prometheus_operator_ready{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\"}[5m]) == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus operator in {{ $labels.namespace }} namespace rejected {{ printf \"%0.0f\" $value }} {{ $labels.controller }}/{{ $labels.resource }} resources.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-prometheusoperatorrejectedresources",
                            "summary": "Resources rejected by Prometheus operator"
                        },
                        "duration": 300,
                        "evaluationTime": 0.000216507,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:16.674697577Z",
                        "name": "PrometheusOperatorRejectedResources",
                        "query": "min_over_time(prometheus_operator_managed_resources{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\",state=\"rejected\"}[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.003310761,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-prometheus.yaml",
                "interval": 30,
                "lastEvaluation": "2020-11-29T15:47:24.859879881Z",
                "name": "prometheus",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to reload its configuration.",
                            "summary": "Failed Prometheus configuration reload."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000380628,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:24.859888352Z",
                        "name": "PrometheusBadConfig",
                        "query": "max_over_time(prometheus_config_last_reload_successful{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) == 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}} is running full.",
                            "summary": "Prometheus alert notification queue predicted to run full in less than 30m."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000229216,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:24.860270541Z",
                        "name": "PrometheusNotificationQueueRunningFull",
                        "query": "(predict_linear(prometheus_notifications_queue_length{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m], 60 * 30) > min_over_time(prometheus_notifications_queue_capacity{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]))",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ printf \"%.1f\" $value }}% errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.",
                            "summary": "Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000235118,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:24.860500705Z",
                        "name": "PrometheusErrorSendingAlertsToSomeAlertmanagers",
                        "query": "(rate(prometheus_notifications_errors_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) / rate(prometheus_notifications_sent_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m])) * 100 > 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ printf \"%.1f\" $value }}% minimum errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to any Alertmanager.",
                            "summary": "Prometheus encounters more than 3% errors sending alerts to any Alertmanager."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000191563,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:24.860736819Z",
                        "name": "PrometheusErrorSendingAlertsToAnyAlertmanager",
                        "query": "min without(alertmanager) (rate(prometheus_notifications_errors_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) / rate(prometheus_notifications_sent_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m])) * 100 > 3",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected to any Alertmanagers.",
                            "summary": "Prometheus is not connected to any Alertmanagers."
                        },
                        "duration": 600,
                        "evaluationTime": 7.6913e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:24.860929058Z",
                        "name": "PrometheusNotConnectedToAlertmanagers",
                        "query": "max_over_time(prometheus_notifications_alertmanagers_discovered{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) < 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} reload failures over the last 3h.",
                            "summary": "Prometheus has issues reloading blocks from disk."
                        },
                        "duration": 14400,
                        "evaluationTime": 0.000231071,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:24.861006558Z",
                        "name": "PrometheusTSDBReloadsFailing",
                        "query": "increase(prometheus_tsdb_reloads_failures_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[3h]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} compaction failures over the last 3h.",
                            "summary": "Prometheus has issues compacting blocks."
                        },
                        "duration": 14400,
                        "evaluationTime": 0.00016283,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:24.861238498Z",
                        "name": "PrometheusTSDBCompactionsFailing",
                        "query": "increase(prometheus_tsdb_compactions_failed_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[3h]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting samples.",
                            "summary": "Prometheus is not ingesting samples."
                        },
                        "duration": 600,
                        "evaluationTime": 7.5949e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:24.861402092Z",
                        "name": "PrometheusNotIngestingSamples",
                        "query": "rate(prometheus_tsdb_head_samples_appended_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) <= 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf \"%.4g\" $value  }} samples/s with different values but duplicated timestamp.",
                            "summary": "Prometheus is dropping samples with duplicate timestamps."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000103577,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:24.861478657Z",
                        "name": "PrometheusDuplicateTimestamps",
                        "query": "rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf \"%.4g\" $value  }} samples/s with timestamps arriving out of order.",
                            "summary": "Prometheus drops samples with out-of-order timestamps."
                        },
                        "duration": 600,
                        "evaluationTime": 8.8821e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:24.861582835Z",
                        "name": "PrometheusOutOfOrderTimestamps",
                        "query": "rate(prometheus_target_scrapes_sample_out_of_order_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to send {{ printf \"%.1f\" $value }}% of the samples to {{ $labels.remote_name}}:{{ $labels.url }}",
                            "summary": "Prometheus fails to send samples to remote storage."
                        },
                        "duration": 900,
                        "evaluationTime": 0.00020974,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:24.861672326Z",
                        "name": "PrometheusRemoteStorageFailures",
                        "query": "(rate(prometheus_remote_storage_failed_samples_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) / (rate(prometheus_remote_storage_failed_samples_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) + rate(prometheus_remote_storage_succeeded_samples_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]))) * 100 > 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write is {{ printf \"%.1f\" $value }}s behind for {{ $labels.remote_name}}:{{ $labels.url }}.",
                            "summary": "Prometheus remote write is behind."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000133023,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:24.861883007Z",
                        "name": "PrometheusRemoteWriteBehind",
                        "query": "(max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) - on(job, instance) group_right() max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m])) > 120",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write desired shards calculation wants to run {{ $value }} shards for queue {{ $labels.remote_name}}:{{ $labels.url }}, which is more than the max of {{ printf `prometheus_remote_storage_shards_max{instance=\"%s\",job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}` $labels.instance | query | first | value }}.",
                            "summary": "Prometheus remote write desired shards calculation wants to run more than configured max shards."
                        },
                        "duration": 900,
                        "evaluationTime": 9.5018e-05,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:24.862016569Z",
                        "name": "PrometheusRemoteWriteDesiredShards",
                        "query": "(max_over_time(prometheus_remote_storage_shards_desired{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) > max_over_time(prometheus_remote_storage_shards_max{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]))",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to evaluate {{ printf \"%.0f\" $value }} rules in the last 5m.",
                            "summary": "Prometheus is failing rule evaluations."
                        },
                        "duration": 900,
                        "evaluationTime": 0.00025931,
                        "health": "ok",
                        "labels": {
                            "severity": "critical"
                        },
                        "lastEvaluation": "2020-11-29T15:47:24.862112091Z",
                        "name": "PrometheusRuleFailures",
                        "query": "increase(prometheus_rule_evaluation_failures_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{ printf \"%.0f\" $value }} rule group evaluations in the last 5m.",
                            "summary": "Prometheus is missing rule evaluations due to slow rule group evaluation."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000614131,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:24.862372039Z",
                        "name": "PrometheusMissingRuleEvaluations",
                        "query": "increase(prometheus_rule_group_iterations_missed_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf \"%.0f\" $value }} targets because the number of targets exceeded the configured target_limit.",
                            "summary": "Prometheus has dropped targets because some scrape configs have exceeded the targets limit."
                        },
                        "duration": 900,
                        "evaluationTime": 0.00019805,
                        "health": "ok",
                        "labels": {
                            "severity": "warning"
                        },
                        "lastEvaluation": "2020-11-29T15:47:24.862989428Z",
                        "name": "PrometheusTargetLimitHit",
                        "query": "increase(prometheus_target_scrape_pool_exceeded_target_limit_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            }
        ]
    },
    "status": "success"
}